{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"UrOIhTeYnA0X","executionInfo":{"status":"ok","timestamp":1713937127740,"user_tz":-330,"elapsed":8,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[],"source":["def create_random_policy(env):\n","     policy = {}\n","     for key in range(0, env.observation_space.n):\n","          current_end = 0\n","          p = {}\n","          for action in range(0, env.action_space.n):\n","               p[action] = 1 / env.action_space.n\n","          policy[key] = p\n","     return policy"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Ihkldm4fnmPn","executionInfo":{"status":"ok","timestamp":1713937127741,"user_tz":-330,"elapsed":8,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[],"source":["def create_state_action_dictionary(env, policy):\n","    Q = {}\n","    for key in policy.keys():\n","         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n","    return Q"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DmC0JUmjnqVl","executionInfo":{"status":"ok","timestamp":1713937127741,"user_tz":-330,"elapsed":7,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[],"source":["def run_game(env, policy, display=True):\n","     env.reset()\n","     episode = []\n","     finished = False\n","\n","     while not finished:\n","          s = env.env.s\n","          if display:\n","               clear_output(True)\n","               env.render()\n","               sleep(1)\n","\n","          timestep = []\n","          timestep.append(s)\n","          n = random.uniform(0, sum(policy[s].values()))\n","          top_range = 0\n","          for prob in policy[s].items():\n","                 top_range += prob[1]\n","                 if n < top_range:\n","                       action = prob[0]\n","                       break\n","          state, reward, finished, info = env.step(action)\n","          timestep.append(action)\n","          timestep.append(reward)\n","\n","          episode.append(timestep)\n","\n","     if display:\n","          clear_output(True)\n","          env.render()\n","          sleep(1)\n","     return episode"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"P-5955M3nzKm","executionInfo":{"status":"ok","timestamp":1713937128650,"user_tz":-330,"elapsed":7,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[],"source":["def test_policy(policy, env):\n","      wins = 0\n","      r = 100\n","      for i in range(r):\n","            w = run_game(env, policy, display=False)[-1][-1]\n","            if w == 1:\n","                  wins += 1\n","      return wins / r"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ydMz9wJ8olDZ","executionInfo":{"status":"ok","timestamp":1713937130314,"user_tz":-330,"elapsed":4,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[],"source":["def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n","    if not policy:\n","        policy = create_random_policy(env)  # Create an empty dictionary to store state action values\n","    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n","    returns = {} # 3.\n","\n","    for _ in range(episodes): # Looping through episodes\n","        G = 0 # Store cumulative reward in G (initialized at 0)\n","        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively\n","\n","        # for loop through reversed indices of episode array.\n","        # The logic behind it being reversed is that the eventual reward would be at the end.\n","        # So we have to go back from the last timestep to the first one propagating result from the future.\n","\n","        for i in reversed(range(0, len(episode))):\n","            s_t, a_t, r_t = episode[i]\n","            state_action = (s_t, a_t)\n","            G += r_t # Increment total reward by reward on current timestep\n","\n","            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: #\n","                if returns.get(state_action):\n","                    returns[state_action].append(G)\n","                else:\n","                    returns[state_action] = [G]\n","\n","                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n","\n","                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n","                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n","                max_Q = random.choice(indices)\n","\n","                A_star = max_Q # 14.\n","\n","                for a in policy[s_t].items(): # Update action probability for s_t in policy\n","                    if a[0] == A_star:\n","                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n","                    else:\n","                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n","    return policy"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GIEkKbawpZFH","outputId":"085fc81b-f50e-4062-a81b-afb52d31b155","executionInfo":{"status":"ok","timestamp":1713937149314,"user_tz":-330,"elapsed":14714,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"]}],"source":["pip install gym\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0Zn8uhChqEBU","executionInfo":{"status":"ok","timestamp":1713937149937,"user_tz":-330,"elapsed":629,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}}},"outputs":[],"source":["import gym"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5lnF-1C8qORP","executionInfo":{"status":"ok","timestamp":1713937149938,"user_tz":-330,"elapsed":11,"user":{"displayName":"AMAN RAJ (RA2111047010105)","userId":"17431646753322584936"}},"outputId":"e7c39d21-3171-4347-aacc-bf7223077997","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r9GcGnX5ompO","outputId":"86c3f38f-29f2-4e3c-ecf8-465e8e7ab7b6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n"]}],"source":["env = gym.make(\"FrozenLake8x8-v1\")\n","policy = monte_carlo_e_soft(env,episodes = 5000)\n","test_policy(policy,env)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
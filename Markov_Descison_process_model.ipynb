{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_FYMH9AiWYk","outputId":"1346002c-8198-4cdd-eefa-9fdc99320212"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal policy: [1, 1, 0, 0, 0]\n","State values: [-2.0661880969000004, -1.0, 10.0, -1.0, -1.0]\n"]}],"source":["# Initialize Markov Decision Process model\n","actions = (0, 1)  # actions (0=left, 1=right)\n","states = (0, 1, 2, 3, 4)  # states (tiles)\n","rewards = [-1, -1, 10, -1, -1]  # Direct rewards per state\n","gamma = 0.9  # discount factor\n","delta = 10  # Error tolerance\n","# Transition probabilities per state-action pair\n","probs = [\n","    [[0.9, 0.1], [0.1, 0.9], [0, 0], [0, 0], [0, 0]],\n","    [[0.9, 0.1], [0, 0], [0.1, 0.9], [0, 0], [0, 0]],\n","    [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]],  # Terminating state (all probs 0)\n","    [[0, 0], [0, 0], [0.9, 0.1], [0, 0], [0.1, 0.9]],\n","    [[0, 0], [0, 0], [0, 0], [0.9, 0.1], [0.1, 0.9]],\n","]\n","\n","# Set policy iteration parameters\n","max_policy_iter = 10000  # Maximum number of policy iterations\n","max_value_iter = 10000  # Maximum number of value iterations\n","pi = [0 for s in states]\n","V = [0 for s in states]\n","\n","for i in range(max_policy_iter):\n","    # Initial assumption: policy is stable\n","    optimal_policy_found = True\n","\n","    # Policy evaluation\n","    # Compute value for each state under current policy\n","    for j in range(max_value_iter):\n","        max_diff = 0\n","        for s in states:\n","            prev_val = V[s]\n","\n","            # Compute state value\n","            V[s] = rewards[s] + gamma * sum(\n","                [prob * V[next_state] for prob, next_state in zip(probs[s][pi[s]], states)])\n","\n","            # Update maximum difference\n","            max_diff = max(max_diff, abs(V[s] - prev_val))\n","\n","        # If diff smaller than threshold delta for all states, algorithm terminates\n","        if max_diff < delta:\n","            break\n","\n","    # Policy iteration\n","    # With updated state values, improve policy if needed\n","    for s in states:\n","        val_max = -float(\"inf\")  # Initialize with negative infinity\n","        new_action = None\n","\n","        # Get direct reward and add discounted downstream values\n","        for a in actions:\n","            val_temp = rewards[s] + gamma * sum(\n","                [prob * V[next_state] for prob, next_state in zip(probs[s][a], states)]\n","            )\n","            if val_temp > val_max:\n","                val_max = val_temp\n","                new_action = a\n","\n","        # Update policy if (i) action improves value and (ii) action different from current policy\n","        if val_max > V[s] and new_action != pi[s]:\n","            optimal_policy_found = False\n","            pi[s] = new_action\n","\n","    # If policy did not change, algorithm terminates\n","    if optimal_policy_found:\n","        break\n","\n","print(\"Optimal policy:\", pi)\n","print(\"State values:\", V)"]}]}